[training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202307171556_1875464464.txt
hive> use hive;
OK
Time taken: 4.472 seconds
hive> SELECT COUNT(*) AS inconsistent_records
    > FROM click c
    > LEFT OUTER JOIN customer cust ON c.userID = cust.userID
    > WHERE cust.userID IS NULL;
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307171258_0012, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307171258_0012
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307171258_0012
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-17 15:57:49,049 Stage-1 map = 0%,  reduce = 0%
2023-07-17 15:57:56,174 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.18 sec
2023-07-17 15:57:57,206 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.96 sec
2023-07-17 15:57:58,226 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.96 sec
2023-07-17 15:57:59,247 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.96 sec
2023-07-17 15:58:00,260 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.96 sec
2023-07-17 15:58:01,273 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.96 sec
2023-07-17 15:58:02,300 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.96 sec
2023-07-17 15:58:03,325 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.33 sec
2023-07-17 15:58:04,359 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.33 sec
2023-07-17 15:58:05,385 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.33 sec
2023-07-17 15:58:06,402 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.33 sec
2023-07-17 15:58:07,425 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.33 sec
MapReduce Total cumulative CPU time: 8 seconds 330 msec
Ended Job = job_202307171258_0012
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307171258_0013, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307171258_0013
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307171258_0013
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-07-17 15:58:13,205 Stage-2 map = 0%,  reduce = 0%
2023-07-17 15:58:19,280 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
2023-07-17 15:58:20,294 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
2023-07-17 15:58:21,317 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
2023-07-17 15:58:22,333 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
2023-07-17 15:58:23,350 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
2023-07-17 15:58:24,362 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
2023-07-17 15:58:25,372 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
2023-07-17 15:58:26,390 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.64 sec
2023-07-17 15:58:27,408 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.64 sec
2023-07-17 15:58:28,419 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.64 sec
2023-07-17 15:58:29,439 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.64 sec
2023-07-17 15:58:30,452 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.64 sec
MapReduce Total cumulative CPU time: 4 seconds 640 msec
Ended Job = job_202307171258_0013
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 8.33 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 4.64 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 12 seconds 970 msec
OK
0
Time taken: 51.762 seconds
hive> select inconsistent_records from click;
FAILED: SemanticException [Error 10004]: Line 1:7 Invalid table alias or column reference 'inconsistent_records': (possible column names are: userid, d, page)
hive> select inconsistent_records from c;    
FAILED: SemanticException [Error 10001]: Line 1:33 Table not found 'c'
hive> select c.inconsistent_records from click;
FAILED: SemanticException [Error 10004]: Line 1:7 Invalid table alias or column reference 'c': (possible column names are: userid, d, page)
hive> SELECT e.userID, e.total_purchase_amount, p.total_amount
    > FROM (
    >   SELECT userID, SUM(amount) AS total_purchase_amount
    >   FROM enriched_data
    >   GROUP BY userID
    > ) e
    > JOIN (
    >   SELECT userID, SUM(amount) AS total_amount
    >   FROM purchase
    >   GROUP BY userID
    > ) p ON e.userID = p.userID
    > WHERE e.total_purchase_amount != p.total_amount;
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307171258_0014, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307171258_0014
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307171258_0014
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-17 16:04:27,752 Stage-1 map = 0%,  reduce = 0%
2023-07-17 16:04:33,847 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.61 sec
2023-07-17 16:04:34,858 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.61 sec
2023-07-17 16:04:35,875 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.61 sec
2023-07-17 16:04:36,889 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.61 sec
2023-07-17 16:04:37,903 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.61 sec
2023-07-17 16:04:38,927 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.61 sec
2023-07-17 16:04:39,943 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.61 sec
2023-07-17 16:04:40,957 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.69 sec
2023-07-17 16:04:41,976 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.69 sec
2023-07-17 16:04:42,996 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.69 sec
2023-07-17 16:04:44,015 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.69 sec
2023-07-17 16:04:45,026 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.69 sec
MapReduce Total cumulative CPU time: 4 seconds 690 msec
Ended Job = job_202307171258_0014
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307171258_0015, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307171258_0015
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307171258_0015
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2023-07-17 16:04:50,758 Stage-3 map = 0%,  reduce = 0%
2023-07-17 16:04:56,822 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.62 sec
2023-07-17 16:04:57,834 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.62 sec
2023-07-17 16:04:58,842 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.62 sec
2023-07-17 16:04:59,856 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.62 sec
2023-07-17 16:05:00,870 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.62 sec
2023-07-17 16:05:01,885 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.62 sec
2023-07-17 16:05:02,903 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.62 sec
2023-07-17 16:05:03,914 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.96 sec
2023-07-17 16:05:04,926 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.96 sec
2023-07-17 16:05:05,950 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.96 sec
2023-07-17 16:05:06,964 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.96 sec
2023-07-17 16:05:07,987 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.96 sec
MapReduce Total cumulative CPU time: 4 seconds 960 msec
Ended Job = job_202307171258_0015
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307171258_0016, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307171258_0016
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307171258_0016
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2023-07-17 16:05:13,814 Stage-2 map = 0%,  reduce = 0%
2023-07-17 16:05:22,923 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.18 sec
2023-07-17 16:05:23,932 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.18 sec
2023-07-17 16:05:24,941 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.18 sec
2023-07-17 16:05:25,955 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.18 sec
2023-07-17 16:05:26,976 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.18 sec
2023-07-17 16:05:27,986 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.18 sec
2023-07-17 16:05:28,995 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.18 sec
2023-07-17 16:05:30,008 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.69 sec
2023-07-17 16:05:31,017 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.69 sec
2023-07-17 16:05:32,026 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.69 sec
2023-07-17 16:05:33,039 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.69 sec
2023-07-17 16:05:34,051 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.69 sec
MapReduce Total cumulative CPU time: 7 seconds 690 msec
Ended Job = job_202307171258_0016
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 4.69 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 4.96 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Job 2: Map: 2  Reduce: 1   Cumulative CPU: 7.69 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 17 seconds 340 msec
OK
1	200	100
2	300	150
3	600	200
4	480	120
5	160	80
Time taken: 72.424 seconds
hive> 

